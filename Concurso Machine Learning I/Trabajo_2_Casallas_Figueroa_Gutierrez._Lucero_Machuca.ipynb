{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56900124",
   "metadata": {},
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandasql import sqldf\n",
    "import warnings\n",
    "from itertools import combinations #Generar combinacion de variables\n",
    "from optbinning import OptimalBinning\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4cb861",
   "metadata": {},
   "source": [
    "# Importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Tabla Trabajo Grupal N°2.xlsx\", sheet_name= \"Desarrollo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ff75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() #Confirmo que no tiene nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e78a1",
   "metadata": {},
   "source": [
    "El grafico anterior más la tabla nos confirmen quen o existen valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bee45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #Data con 12.356 registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique() #Id demuestra que la data no tiene registros repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Default\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b111660",
   "metadata": {},
   "source": [
    "Al tener los mismos ID distintos versus el total de registros nos muestra que no hay registros duplicados  \n",
    "Además vemos que Nivel educacional es la unica variable realmente categorica además de Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74829a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[df.columns[1:-1]], df[\"Default\"],\n",
    "                                                    train_size= 0.7,\n",
    "                                                    random_state = 7)\n",
    "\n",
    "df_train = pd.concat([x_train, y_train], axis = 1)\n",
    "df_test = pd.concat([x_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42563195",
   "metadata": {},
   "source": [
    "# Analisis descriptivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e61ae00",
   "metadata": {},
   "source": [
    "## Analisis variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6aad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_histograma(df, variable):\n",
    "    plt.figure(figsize = (7,4))\n",
    "    sns.histplot(df, x = variable)\n",
    "    plt.title(f\"{variable}\")\n",
    "    plt.xlabel(f\"{variable}\")\n",
    "    plt.ylabel(\"Cantidad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c085dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf50cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c35286",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16303097",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_histograma(df_train, df_train.columns[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(df_train.select_dtypes(include=['number']).corr(), annot= True, cmap = \"coolwarm\")\n",
    "plt.title(\"Matriz de correlacion de variables numericas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ca5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_histograma_subplots(df, variable, ax ):\n",
    "    sns.histplot(df, x = variable, ax = ax)\n",
    "    plt.title(f\"{variable}\")\n",
    "    plt.xlabel(f\"{variable}\")\n",
    "    plt.ylabel(\"Cantidad\")\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize = (20,10))\n",
    "axes = axes.flatten()\n",
    "for ax, var in zip(axes, df_train.columns):\n",
    "    graficar_histograma_subplots(df_train, var, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Histograma de Variables (Entrenamiento)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62cc54",
   "metadata": {},
   "source": [
    "## Analisis variables vs objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5029270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_histograma_subplots(df, variable, ax ):\n",
    "    sns.histplot(df, x = variable, hue = \"Default\" , ax = ax)\n",
    "    plt.title(f\"{variable}\")\n",
    "    plt.xlabel(f\"{variable}\")\n",
    "    plt.ylabel(\"Cantidad\")\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize = (35,36))\n",
    "axes = axes.flatten()\n",
    "for ax, var in zip(axes, df_train.columns):\n",
    "    graficar_histograma_subplots(df_train, var, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Histograma de Variables (Entrenamiento)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_train, hue = \"Default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8f863",
   "metadata": {},
   "source": [
    "#  Modelado Exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712467d",
   "metadata": {},
   "source": [
    "## Modelo Base - Sin agregados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbcbac3",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded29621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_backup = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ed65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Nivel_Educacional\"].nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.get_dummies(df_train, \"Nivel_Educacional\", drop_first= True)\n",
    "df_test =pd.get_dummies(df_test, \"Nivel_Educacional\", drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de169044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [col for col in df_train.columns if col not in [\"Default\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e49a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train[variables]\n",
    "y_terain = df_train[\"Default\"]\n",
    "x_test = df_test[variables]\n",
    "y_test = df_test[\"Default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa80d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = LogisticRegression()\n",
    "\n",
    "RL.fit(x_train, y_train)\n",
    "y_predic = RL.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_predic)\n",
    "print(\"El accuracy del modelo es\")\n",
    "print(f\"El Accuracy del modelo es {acc:.3f}\")\n",
    "\n",
    "\n",
    "coef = RL.coef_[0]\n",
    "intercepto = RL.intercept_[0]\n",
    "\n",
    "df_coef = pd.DataFrame({\n",
    "    \"Variable\": variables,\n",
    "    \"Coeficiente\": coef\n",
    "})\n",
    "\n",
    "print(\"Intercepto:\", intercepto)\n",
    "print(df_coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = RL.predict_proba(x_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "plt.xlabel(\"Tasa de Falsos Positivos (1 - Especificidad)\")\n",
    "plt.ylabel(\"Tasa de Verdaderos Positivos (Sensibilidad)\")\n",
    "plt.title(\"Curva ROC - Regresión Logística\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e57791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = np.arange(0, 1.01, 0.01)   # thresholds de 0.00 a 1.00\n",
    "accuracies = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_scores >= t).astype(int)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_t))\n",
    "\n",
    "best_t = thresholds[np.argmax(accuracies)]\n",
    "best_acc = max(accuracies)\n",
    "\n",
    "print(f\"Mejor threshold = {best_t:.2f}\")\n",
    "print(f\"Accuracy máximo = {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e034107",
   "metadata": {},
   "source": [
    "### Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d46580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_stepwise(x_train, x_test, y_train, y_test, direccion):\n",
    "    modelo = LogisticRegression()\n",
    "    stepwise = SequentialFeatureSelector(\n",
    "        modelo,\n",
    "        direction = direccion,\n",
    "        scoring = \"accuracy\",\n",
    "        cv=5,\n",
    "\n",
    "    )\n",
    "\n",
    "    stepwise.fit(x_train, y_train)\n",
    "    filtro = stepwise.get_support()\n",
    "    col_stepwise = x_train.columns[filtro]\n",
    "    modelo.fit(x_train[col_stepwise], y_train)\n",
    "    y_pred = modelo.predict(x_test[col_stepwise])\n",
    "\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f\"Valor de Accuracy: {accuracy}\")\n",
    "    print(\"Variables seleccionadas con RFE:\")\n",
    "    print(col_stepwise)\n",
    "    \n",
    "    coef = modelo.coef_[0]\n",
    "    intercepto = modelo.intercept_[0]\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "    \"Variable\": col_stepwise,\n",
    "    \"Coeficiente\": coef})\n",
    "\n",
    "    print(\"Intercepto:\", intercepto)\n",
    "    print(df_coef)\n",
    "\n",
    "\n",
    "modelo_stepwise(x_train, x_test, y_train, y_test, \"backward\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1257c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89951923",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_stepwise(x_train, x_test, y_train, y_test, \"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a863e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d68502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nuevo = x_train.copy()\n",
    "x_test_nuevo = x_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb04bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nuevo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_num = [\"Edad\", \"Años_Trabajando\",\"Ingresos\", \"Deuda_Comercial\", \"Deuda_Credito\", \"Otras_Deudas\", \"Ratio_Ingresos_Deudas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53acb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nuevo[variables_num] = np.log( x_train_nuevo[variables_num] +1)\n",
    "x_test_nuevo[variables_num] = np.log( x_test_nuevo[variables_num] +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_histograma_subplots(df, variable, ax ):\n",
    "    sns.histplot(df, x = variable, ax = ax)\n",
    "    plt.title(f\"{variable}\")\n",
    "    plt.xlabel(f\"{variable}\")\n",
    "    plt.ylabel(\"Cantidad\")\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize = (20,10))\n",
    "axes = axes.flatten()\n",
    "for ax, var in zip(axes, df_train.columns):\n",
    "    graficar_histograma_subplots(df_train, var, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Histograma de Variables (Entrenamiento)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficar_histograma_subplots(df, variable, ax ):\n",
    "    sns.histplot(df, x = variable, ax = ax)\n",
    "    plt.title(f\"{variable}\")\n",
    "    plt.xlabel(f\"{variable}\")\n",
    "    plt.ylabel(\"Cantidad\")\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize = (20,10))\n",
    "axes = axes.flatten()\n",
    "for ax, var in zip(axes, df_train_nuevo.columns):\n",
    "    graficar_histograma_subplots(df_train_nuevo, var, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Histograma de Variables (Entrenamiento)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228421bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_stepwise(x_train_nuevo, x_test_nuevo, y_train, y_test, \"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_stepwise(x_train_nuevo, x_test_nuevo, y_train, y_test, \"forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb97e09",
   "metadata": {},
   "source": [
    "## Modelo con ponderacion de pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436cf8e3",
   "metadata": {},
   "source": [
    "### Forma Profesor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_train = y_train.mean()\n",
    "peso = media_train/(1-media_train)\n",
    "peso_train = np.where(y_train ==1, 1, peso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = LogisticRegression()\n",
    "\n",
    "RL.fit(x_train, y_train, sample_weight=peso_train)\n",
    "y_predic = RL.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_predic)\n",
    "print(\"El accuracy del modelo es\")\n",
    "print(f\"El Accuracy del modelo es {acc:.3f}\")\n",
    "\n",
    "\n",
    "coef = RL.coef_[0]\n",
    "intercepto = RL.intercept_[0]\n",
    "\n",
    "df_coef = pd.DataFrame({\n",
    "    \"Variable\": variables,\n",
    "    \"Coeficiente\": coef\n",
    "})\n",
    "\n",
    "print(\"Intercepto:\", intercepto)\n",
    "print(df_coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecae14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_stepwise(x_train, x_test, y_train, y_test, direccion):\n",
    "    modelo = LogisticRegression()\n",
    "    stepwise = SequentialFeatureSelector(\n",
    "        modelo,\n",
    "        direction = direccion,\n",
    "        scoring = \"accuracy\",\n",
    "        cv=5,\n",
    "\n",
    "    )\n",
    "\n",
    "    stepwise.fit(x_train, y_train)\n",
    "    filtro = stepwise.get_support()\n",
    "    col_stepwise = x_train.columns[filtro]\n",
    "    modelo.fit(x_train[col_stepwise], y_train, sample_weight= peso_train)\n",
    "    y_pred = modelo.predict(x_test[col_stepwise])\n",
    "\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f\"Valor de Accuracy: {accuracy}\")\n",
    "    print(\"Variables seleccionadas con RFE:\")\n",
    "    print(col_stepwise)\n",
    "    \n",
    "    coef = modelo.coef_[0]\n",
    "    intercepto = modelo.intercept_[0]\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "    \"Variable\": col_stepwise,\n",
    "    \"Coeficiente\": coef})\n",
    "\n",
    "    print(\"Intercepto:\", intercepto)\n",
    "    print(df_coef)\n",
    "\n",
    "    return  modelo, col_stepwise\n",
    "\n",
    "\n",
    "modelo, col_selecc = modelo_stepwise(x_train, x_test, y_train, y_test, \"backward\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = modelo.predict_proba(x_test[col_selecc])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "plt.xlabel(\"Tasa de Falsos Positivos (1 - Especificidad)\")\n",
    "plt.ylabel(\"Tasa de Verdaderos Positivos (Sensibilidad)\")\n",
    "plt.title(\"Curva ROC - Regresión Logística\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a29dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Crear otro threshold\n",
    "\n",
    "thresholds = np.arange(0, 1.01, 0.01)   # thresholds de 0.00 a 1.00\n",
    "accuracies = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_scores >= t).astype(int)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_t))\n",
    "\n",
    "best_t = thresholds[np.argmax(accuracies)]\n",
    "best_acc = max(accuracies)\n",
    "\n",
    "print(f\"Mejor threshold = {best_t:.2f}\")\n",
    "print(f\"Accuracy máximo = {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f60045",
   "metadata": {},
   "source": [
    "### Forma balanceo recomendada Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(x_train)\n",
    "N1 = y_train.sum()\n",
    "N0 = N-N1\n",
    "\n",
    "w0 = N/(2*N0)\n",
    "w1 = N/(2*N1)\n",
    "\n",
    "peso_train2 = np.where(y_train==1, w1,w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7588d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = LogisticRegression()\n",
    "\n",
    "RL.fit(x_train, y_train, sample_weight=peso_train2)\n",
    "y_predic = RL.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_predic)\n",
    "print(\"El accuracy del modelo es\")\n",
    "print(f\"El Accuracy del modelo es {acc:.3f}\")\n",
    "\n",
    "\n",
    "coef = RL.coef_[0]\n",
    "intercepto = RL.intercept_[0]\n",
    "\n",
    "df_coef = pd.DataFrame({\n",
    "    \"Variable\": variables,\n",
    "    \"Coeficiente\": coef\n",
    "})\n",
    "\n",
    "print(\"Intercepto:\", intercepto)\n",
    "print(df_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_stepwise(x_train, x_test, y_train, y_test, direccion):\n",
    "    modelo = LogisticRegression()\n",
    "    stepwise = SequentialFeatureSelector(\n",
    "        modelo,\n",
    "        direction = direccion,\n",
    "        scoring = \"accuracy\",\n",
    "        cv=5,\n",
    "\n",
    "    )\n",
    "\n",
    "    stepwise.fit(x_train, y_train)\n",
    "    filtro = stepwise.get_support()\n",
    "    col_stepwise = x_train.columns[filtro]\n",
    "    modelo.fit(x_train[col_stepwise], y_train, sample_weight= peso_train2)\n",
    "    y_pred = modelo.predict(x_test[col_stepwise])\n",
    "\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f\"Valor de Accuracy: {accuracy}\")\n",
    "    print(\"Variables seleccionadas con RFE:\")\n",
    "    print(col_stepwise)\n",
    "    \n",
    "    coef = modelo.coef_[0]\n",
    "    intercepto = modelo.intercept_[0]\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "    \"Variable\": col_stepwise,\n",
    "    \"Coeficiente\": coef})\n",
    "\n",
    "    print(\"Intercepto:\", intercepto)\n",
    "    print(df_coef)\n",
    "\n",
    "\n",
    "modelo_stepwise(x_train, x_test, y_train, y_test, \"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definimos qué columnas vamos a binnear ---\n",
    "cols_binning = [\n",
    "    \"Edad\", \"Años_Trabajando\", \"Ingresos\",\n",
    "    \"Deuda_Comercial\", \"Deuda_Credito\",\n",
    "    \"Otras_Deudas\", \"Ratio_Ingresos_Deudas\"\n",
    "]\n",
    "\n",
    "cols_dummies = [\n",
    "    \"Nivel_Educacional_Med\", \"Nivel_Educacional_Posg\",\n",
    "    \"Nivel_Educacional_SupCom\", \"Nivel_Educacional_SupInc\"\n",
    "]\n",
    "\n",
    "# Número de bins\n",
    "N_BINS = 5  \n",
    "\n",
    "# Para guardar los bordes de cada bin\n",
    "bin_edges = {}\n",
    "\n",
    "# --- 2. Hacemos binning SOLO en x_train ---\n",
    "x_train_binned = x_train.copy()\n",
    "\n",
    "for col in cols_binning:\n",
    "    # Generamos bins por cuantiles en TRAIN\n",
    "    x_train_binned[col + \"_bin\"], edges = pd.qcut(\n",
    "        x_train[col],\n",
    "        q=N_BINS,\n",
    "        retbins=True,\n",
    "        duplicates=\"drop\"\n",
    "    )\n",
    "    bin_edges[col] = edges  # Guardamos bordes exactos\n",
    "\n",
    "# --- 3. Función que aplica los mismos bins a cualquier dataset ---\n",
    "def aplicar_binning(df, cols_binning, bin_edges):\n",
    "    df_binned = df.copy()\n",
    "    for col in cols_binning:\n",
    "        edges = bin_edges[col]  # bordes del train\n",
    "        df_binned[col + \"_bin\"] = pd.cut(\n",
    "            df[col],\n",
    "            bins=edges,\n",
    "            include_lowest=True\n",
    "        )\n",
    "    return df_binned\n",
    "\n",
    "# --- 4. Aplicamos binning a x_test usando los bordes del train ---\n",
    "x_test_binned = aplicar_binning(x_test, cols_binning, bin_edges)\n",
    "\n",
    "# --- 5. Dejamos SOLO las columnas necesarias ---\n",
    "cols_final = [c + \"_bin\" for c in cols_binning] + cols_dummies\n",
    "\n",
    "x_train_model = x_train_binned[cols_final]\n",
    "x_test_model  = x_test_binned[cols_final]\n",
    "\n",
    "print(\"Train binned:\")\n",
    "print(x_train_model.head())\n",
    "\n",
    "print(\"\\nTest binned:\")\n",
    "print(x_test_model.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_binn = x_test_model.columns[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04cba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd31a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_model= pd.get_dummies(x_train_model[col_binn], drop_first= True)\n",
    "x_test_model =pd.get_dummies(x_test_model[col_binn] , drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed596fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_binned = LogisticRegression()\n",
    "\n",
    "modelo_binned.fit(x_train_model, y_train)\n",
    "y_pred_binned = modelo_binned.predict(x_test_model)\n",
    "accuracy = accuracy_score(y_test, y_pred_binned)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Modelo base\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Número de variables\n",
    "n_features = x_train_model.shape[1]\n",
    "\n",
    "mejor_acc = 0\n",
    "mejores_vars = None\n",
    "\n",
    "# IMPORTANTE: backward permite seleccionar desde n_features-1 hasta 1\n",
    "for k in range(1, n_features):\n",
    "    print(f\"Probando con {k} variables...\")\n",
    "\n",
    "    # Backward en sklearn\n",
    "    sbs = SequentialFeatureSelector(\n",
    "        logreg,\n",
    "        n_features_to_select=k,\n",
    "        direction=\"backward\",\n",
    "        scoring=\"accuracy\",  # puedes usar \"roc_auc\"\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    sbs.fit(x_train_model, y_train)\n",
    "\n",
    "    # Variables seleccionadas\n",
    "    selected_mask = sbs.get_support()\n",
    "    selected_features = x_train_model.columns[selected_mask]\n",
    "\n",
    "    # Entrenamos modelo con esas variables\n",
    "    modelo_backward = LogisticRegression(max_iter=1000)\n",
    "    modelo_backward.fit(x_train_model[selected_features], y_train)\n",
    "\n",
    "    # Evaluamos en test\n",
    "    y_pred = modelo_backward.predict(x_test_model[selected_features])\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy con {k} variables: {acc:.5f}\")\n",
    "\n",
    "    # Guardar el mejor resultado\n",
    "    if acc > mejor_acc:\n",
    "        mejor_acc = acc\n",
    "        mejores_vars = selected_features\n",
    "\n",
    "print(\"\\n====================================\")\n",
    "print(\"✅ Mejor resultado Backward Stepwise\")\n",
    "print(\"====================================\")\n",
    "print(f\"Accuracy: {mejor_acc:.5f}\")\n",
    "print(\"Variables seleccionadas:\")\n",
    "print(list(mejores_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8063eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"Tabla Trabajo Grupal N°2.xlsx\", sheet_name= \"Desarrollo\")\n",
    "\n",
    "df.drop([\"Id_Cliente\"], axis=1, inplace=True)\n",
    "df.drop([\"Ratio_Ingresos_Deudas\"], axis=1, inplace=True)\n",
    "df.drop([\"Nivel_Educacional\"], axis=1, inplace=True)\n",
    "\n",
    "# Dropeamos el ratio ingersos deduas, ya que se volverán a generar al momento de generar ratios\n",
    "df[\"Total_Deudas\"] = df[\"Deuda_Comercial\"] + df[\"Deuda_Credito\"] + df[\"Otras_Deudas\"]\n",
    "target = 'Default'\n",
    "\n",
    "# Generar Ratios\n",
    "col_continuas = [col for col in df.columns if df[col].dtype in [\"int64\", \"float64\"] and col != target]\n",
    "\n",
    "def combinacion_de_variables(df, variables):\n",
    "    df_2 = df.copy()\n",
    "    eps = 1e-6 \n",
    "    for x, y in combinations(variables, 2):\n",
    "        df_2[f\"Prod_{x}_{y}\"] = df[x] * df[y]\n",
    "        df_2[f\"Ratio_{x}_{y}\"] = df[x] / (df[y] + eps)\n",
    "        df_2[f\"Ratio_{y}_{x}\"] = df[y] / (df[x] + eps)\n",
    "    return df_2\n",
    "\n",
    "df = combinacion_de_variables(df, col_continuas)\n",
    "\n",
    "# Aplicacion del split en crudo por ahroa, ya que posteriormente se pasaran a woe\n",
    "x_en_bruto = df.drop(columns=[target])\n",
    "y_en_bruto = df[target]\n",
    "x_train_bruto, X_test_bruto, y_train, y_test = train_test_split(x_en_bruto, y_en_bruto, test_size=0.3, random_state=42, stratify=y_en_bruto)\n",
    "\n",
    "# Aplicacion del woe e IV\n",
    "def crear_woes(x_train, target):\n",
    "    \"\"\"Calcula WOE sobre una serie y devuelve el mapa y los bins usados\"\"\"\n",
    "    _, bins = pd.qcut(x_train, q=10, duplicates='drop', retbins=True)\n",
    "    bins[0] = -np.inf\n",
    "    bins[-1] = np.inf\n",
    "\n",
    "    # Aplicar bins para calcular WOE\n",
    "    bineado = pd.cut(x_train, bins=bins).astype(str)\n",
    "\n",
    "    # Tabla WOE\n",
    "    df_temp = pd.DataFrame({'bin': bineado, 'target': target})\n",
    "    grouped = df_temp.groupby('bin')['target'].agg(['count', 'sum'])\n",
    "    grouped['malos_pagadores'] = grouped['sum'].replace(0, 0.5)\n",
    "    grouped['buenos_pagadores'] = (grouped['count'] - grouped['sum']).replace(0, 0.5)\n",
    "    \n",
    "    total_malos = target.sum()\n",
    "    total_buenos = target.count() - total_malos\n",
    "    \n",
    "    grouped['WOE'] = np.log((grouped['buenos_pagadores'] / total_buenos) / (grouped['malos_pagadores'] / total_malos))\n",
    "    grouped['IV'] = (grouped['buenos_pagadores']/total_buenos - grouped['malos_pagadores']/total_malos) * grouped['WOE']\n",
    "    \n",
    "    return grouped['WOE'].to_dict(), grouped['IV'].sum(), bins\n",
    "\n",
    "def transformar_a_woe(x_train, tabla_woes, bins):\n",
    "    \"\"\"Aplica el WOE a nuevos datos\"\"\"\n",
    "    bineado = pd.cut(x_train, bins=bins).astype(str)\n",
    "    return bineado.map(tabla_woes).fillna(0) # Si hay nulos (categoría nueva), WOE 0\n",
    "\n",
    "# --- Aplicar transformación\n",
    "print(\"Calculando WOE\")\n",
    "\n",
    "X_train_woe = pd.DataFrame(index=x_train_bruto.index)\n",
    "X_test_woe = pd.DataFrame(index=X_test_bruto.index)\n",
    "iv_resumen = {}\n",
    "\n",
    "for col in x_train_bruto.columns:\n",
    "    # acalculo woe solo al train, evitamos fuga de informacion\n",
    "    tabla_woes, iv, bins = crear_woes(x_train_bruto[col], y_train)\n",
    "    iv_resumen[col] = iv\n",
    "        \n",
    "    # luego del calculo sobre test se aplica el woe\n",
    "    X_train_woe[col] = transformar_a_woe(x_train_bruto[col], tabla_woes, bins)\n",
    "    X_test_woe[col] = transformar_a_woe(X_test_bruto[col], tabla_woes, bins)\n",
    "\n",
    "print(f\"Variables transformadas: {X_train_woe.shape[1]}\")\n",
    "\n",
    "#ocupamos el IV para descartar variables con IV bajo\n",
    "iv_umbral = 0.01\n",
    "variables_fuertes_iv = [k for k, v in iv_resumen.items() if v >= iv_umbral]\n",
    "print(f\"Variables con IV > {iv_umbral}: {len(variables_fuertes_iv)}\")\n",
    "\n",
    "X_train_woe = X_train_woe[variables_fuertes_iv]\n",
    "X_test_woe = X_test_woe[variables_fuertes_iv]\n",
    "\n",
    "\n",
    "#APLICACION DE ESTRATEGIAS\n",
    "\n",
    "# --- A) FORWARD\n",
    "def aplicar_forward(X_tr, y_tr, X_te, y_te):\n",
    "    inicial, restante = [], list(X_tr.columns)\n",
    "    mejor_accuracy = 0\n",
    "    print(\"\\n----- Forward -----\")\n",
    "    while restante:\n",
    "        cv_prob = []\n",
    "        for col in restante:\n",
    "            m = LogisticRegression(solver='liblinear', random_state=42)\n",
    "            m.fit(X_tr[inicial + [col]], y_tr)\n",
    "            cv_prob.append((accuracy_score(y_te, m.predict(X_te[inicial + [col]])), col))\n",
    "        cv_prob.sort(reverse=True)\n",
    "        if cv_prob[0][0] > mejor_accuracy:\n",
    "            mejor_accuracy, mejor_col = cv_prob[0]\n",
    "            inicial.append(mejor_col)\n",
    "            restante.remove(mejor_col)\n",
    "            print(f\"Agregada: {cv_prob[0][1]} (Acc: {mejor_accuracy:.4f})\")\n",
    "        else: break\n",
    "    return inicial, mejor_accuracy\n",
    "\n",
    "# --- B) BACKWARD CON TOLERANCIA\n",
    "def aplicar_backward(X_tr, y_tr, X_te, y_te, tolerancia=0.001):\n",
    "    \"\"\"\n",
    "    Elimina variables si el accuracy no baja más que la torelancia.\n",
    "    tolerancia=0.001 significa que aceptamos perder un 0.01% de accuracy a cambio de quitar una variable.\n",
    "    \"\"\"\n",
    "    cols = list(X_tr.columns)\n",
    "    m = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    m.fit(X_tr[cols], y_tr)\n",
    "    mejor_accuracy = accuracy_score(y_te, m.predict(X_te[cols]))\n",
    "    \n",
    "    print(f\"\\n--- Backward (Tolerancia: {tolerancia}) ---\")\n",
    "    print(f\"Inicio: {len(cols)} vars | Acc: {mejor_accuracy:.4f}\")\n",
    "    \n",
    "    loop = True\n",
    "    while loop and len(cols) > 1:\n",
    "        cv_prob = []\n",
    "        # Evaluar qué pasa si quitamos cada variable\n",
    "        for col in cols:\n",
    "            temporal_col = [f for f in cols if f != col]\n",
    "            m.fit(X_tr[temporal_col], y_tr)\n",
    "            acc = accuracy_score(y_te, m.predict(X_te[temporal_col]))\n",
    "            cv_prob.append((acc, col))\n",
    "        \n",
    "        cv_prob.sort(reverse=True)\n",
    "        accuracy_sin_var, variables_quitadas = cv_prob[0]\n",
    "        \n",
    "        # Si el accuracy al quitar la variable es MAYOR o IGUAL a (Accuracy Actual - Tolerancia)\n",
    "        # Significa que la pérdida es aceptable, así que la quitamos\n",
    "        if accuracy_sin_var >= (mejor_accuracy - tolerancia):\n",
    "            cols.remove(variables_quitadas)\n",
    "            # Solo actualizamos el mejor_accuracy si realmente mejoró\n",
    "            # pero aceptamos el nuevo estado más simple.\n",
    "            if accuracy_sin_var > mejor_accuracy:\n",
    "                mejor_accuracy = accuracy_sin_var\n",
    "            print(f\"Eliminada: {variables_quitadas} (Acc: {accuracy_sin_var:.4f})\")\n",
    "        else:\n",
    "            print(\"No se puede eliminar más sin sacrificar accuracy\")\n",
    "            loop = False\n",
    "            \n",
    "    return cols, mejor_accuracy\n",
    "\n",
    "# --- C) STEPWISE BIDIRECCIONAL---\n",
    "def stepwise_bidireccional(X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"Agrega la mejor, luego intenta eliminar la peor\"\"\"\n",
    "    col_actual = []\n",
    "    restante = list(X_tr.columns)\n",
    "    mejor_accuracy = 0\n",
    "    \n",
    "    print(\"\\n----- Stepwise bidireccional-----\")\n",
    "    while restante:\n",
    "        # 1. Paso Forward\n",
    "        acc_agregados = []\n",
    "        for col in restante:\n",
    "            m = LogisticRegression(solver='liblinear', random_state=42)\n",
    "            m.fit(X_tr[col_actual + [col]], y_tr)\n",
    "            acc = accuracy_score(y_te, m.predict(X_te[col_actual + [col]]))\n",
    "            acc_agregados.append((acc, col))\n",
    "        acc_agregados.sort(reverse=True)\n",
    "        \n",
    "        if acc_agregados[0][0] > mejor_accuracy:\n",
    "            mejor_accuracy, agregado = acc_agregados[0]\n",
    "            col_actual.append(agregado)\n",
    "            restante.remove(agregado)\n",
    "            print(f\"Agregada: {agregado} (Acc: {mejor_accuracy:.4f})\")\n",
    "            \n",
    "            # 2. Paso Backward (quita variables)\n",
    "            # Revisamos si alguna de las QUE YA TENEMOS sobra\n",
    "            if len(col_actual) > 2:\n",
    "                acc_quitados = []\n",
    "                for col in col_actual:\n",
    "                    temp = [f for f in col_actual if f != col]\n",
    "                    m.fit(X_tr[temp], y_tr)\n",
    "                    acc = accuracy_score(y_te, m.predict(X_te[temp]))\n",
    "                    acc_quitados.append((acc, col))\n",
    "                acc_quitados.sort(reverse=True)\n",
    "                \n",
    "                # Si al quitar una mejoramos el mejor_accuracy actual, la sacamos\n",
    "                if acc_quitados[0][0] > mejor_accuracy:\n",
    "                    mejor_accuracy, quitados = acc_quitados[0]\n",
    "                    col_actual.remove(quitados)\n",
    "                    restante.append(quitados)\n",
    "                    print(f\"Eliminada por redundancia: {quitados} (Acc subió a: {mejor_accuracy:.4f})\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return col_actual, mejor_accuracy\n",
    "\n",
    "\n",
    "mejor_fwd, acc_fwd = aplicar_forward(X_train_woe, y_train, X_test_woe, y_test)\n",
    "\n",
    "mejor_bwd, acc_bwd = aplicar_backward(X_train_woe, y_train, X_test_woe, y_test, tolerancia=0.0005)\n",
    "\n",
    "mejor_bidir, acc_bidir = stepwise_bidireccional(X_train_woe, y_train, X_test_woe, y_test)\n",
    "\n",
    "# --- RESULTADOS\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" COMPARATIVA FINAL\")\n",
    "print(\"=\"*30)\n",
    "print(f\"1. Forward:        {acc_fwd:.4f} ({len(mejor_fwd)} variables)\")\n",
    "print(f\"2. Backward:       {acc_bwd:.4f} ({len(mejor_bwd)} variables)\")\n",
    "print(f\"3. Bidireccional:  {acc_bidir:.4f} ({len(mejor_bidir)} va   riables)\")\n",
    "\n",
    "# guarda y luego muestra resultados de los intentos y los ordenamos de mayor a menor\n",
    "resultados = [\n",
    "    ('Forward', acc_fwd, len(mejor_fwd), mejor_fwd),\n",
    "    ('Backward Smart', acc_bwd, len(mejor_bwd), mejor_bwd),\n",
    "    ('Bidireccional', acc_bidir, len(mejor_bidir), mejor_bidir)\n",
    "]\n",
    "\n",
    "# Ordenamos por Accuracy descendente\n",
    "resultados.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "nombre_ganador, acc_ganador, ganadores_var_len, variables_ganadores = resultados[0]\n",
    "print(f\"\\n>>> Estrategia ganadora: {nombre_ganador}\")\n",
    "\n",
    "# ENTRENAMIENTO FINAL\n",
    "modelo_final = LogisticRegression(solver='liblinear', random_state=42)\n",
    "modelo_final.fit(X_train_woe[variables_ganadores], y_train)\n",
    "\n",
    "# Optimización Threshold\n",
    "probs = modelo_final.predict_proba(X_test_woe[variables_ganadores])[:, 1]\n",
    "mejor_t = 0.5; max_acc = 0\n",
    "for t in np.arange(0.1, 0.9, 0.01):\n",
    "    acc = accuracy_score(y_test, (probs >= t).astype(int))\n",
    "    if acc > max_acc: max_acc = acc; mejor_t = t\n",
    "\n",
    "print(f\"Accuracy Optimizado: {max_acc:.4f} (Threshold: {mejor_t:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab000554",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_continuas = [col for col in df_train.columns[:-1] if df_train[col].dtype in [\"int64\", \"float64\"]]\n",
    "\n",
    "def combinacion_de_variables(df, variables):\n",
    "    df_2 = df.copy()\n",
    "    for x,y in combinations(variables, 2): \n",
    "        df_2[f\"Prod_{x}_{y}\"] = df[x]*df[y]\n",
    "        df_2[f\"Ratio_{x}_{y}\"] = df[x] / (df[y] + 0.00000000001)\n",
    "\n",
    "\n",
    "    return df_2\n",
    "\n",
    "df_train_total = combinacion_de_variables(df_train, col_continuas)\n",
    "df_test_total = combinacion_de_variables(df_test, col_continuas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_numericas_total = [col for col in df_train_total.columns if col not in [\"Default\", \"Nivel_Educacional\"]]\n",
    "variables_numericas_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7582eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_backward = ['Ingresos', 'Deuda_Credito', 'Prod_Edad_Deuda_Comercial',\n",
    "       'Prod_Edad_Otras_Deudas', 'Ratio_Edad_Otras_Deudas',\n",
    "       'Prod_Edad_Ratio_Ingresos_Deudas', 'Ratio_Edad_Ratio_Ingresos_Deudas',\n",
    "       'Prod_Años_Trabajando_Ingresos', 'Prod_Años_Trabajando_Deuda_Comercial',\n",
    "       'Prod_Años_Trabajando_Deuda_Credito',\n",
    "       'Ratio_Años_Trabajando_Deuda_Credito',\n",
    "       'Prod_Años_Trabajando_Otras_Deudas',\n",
    "       'Ratio_Años_Trabajando_Otras_Deudas',\n",
    "       'Prod_Años_Trabajando_Ratio_Ingresos_Deudas',\n",
    "       'Prod_Ingresos_Deuda_Credito', 'Prod_Ingresos_Otras_Deudas',\n",
    "       'Ratio_Ingresos_Ratio_Ingresos_Deudas',\n",
    "       'Ratio_Deuda_Comercial_Deuda_Credito',\n",
    "       'Ratio_Deuda_Comercial_Otras_Deudas',\n",
    "       'Prod_Deuda_Comercial_Ratio_Ingresos_Deudas',\n",
    "       'Prod_Deuda_Credito_Ratio_Ingresos_Deudas',\n",
    "       'Ratio_Deuda_Credito_Ratio_Ingresos_Deudas',\n",
    "       'Prod_Otras_Deudas_Ratio_Ingresos_Deudas',\n",
    "       'Ratio_Otras_Deudas_Ratio_Ingresos_Deudas', 'Años_Trabajando', 'Deuda_Comercial',\n",
    "       'Otras_Deudas'] #Variables que resultaron de iterar todas las variables en un modelo Backward, debido al tiempo de iteracion (Mayor a 2 horas) es que decidimos tomar ese resultado y dejarlo en un arreglo dado que para dejar el codigo con la iteracion nuevamente tendrìamos que volver a dejar prendido un PC por 1 hora\n",
    "\n",
    "x_train = df_train_total[variables_backward]\n",
    "y_train = df_train_total[\"Default\"]\n",
    "\n",
    "x_test = df_test_total[variables_backward]\n",
    "y_test = df_test_total[\"Default\"]\n",
    "\n",
    "\n",
    "def modelo_stepwise(x_train, x_test, y_train, y_test, direccion):\n",
    "    modelo = LogisticRegression()\n",
    "    stepwise = SequentialFeatureSelector(\n",
    "        modelo,\n",
    "        direction = direccion,\n",
    "        scoring = \"accuracy\",\n",
    "        cv=5,\n",
    "\n",
    "    )\n",
    "\n",
    "    stepwise.fit(x_train, y_train)\n",
    "    filtro = stepwise.get_support()\n",
    "    col_stepwise = x_train.columns[filtro]\n",
    "    modelo.fit(x_train[col_stepwise], y_train)\n",
    "    y_pred = modelo.predict(x_test[col_stepwise])\n",
    "\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f\"Valor de Accuracy: {accuracy}\")\n",
    "    print(\"Variables seleccionadas con RFE:\")\n",
    "    print(col_stepwise)\n",
    "    \n",
    "    coef = modelo.coef_[0]\n",
    "    intercepto = modelo.intercept_[0]\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "    \"Variable\": col_stepwise,\n",
    "    \"Coeficiente\": coef})\n",
    "\n",
    "    print(\"Intercepto:\", intercepto)\n",
    "    print(df_coef)\n",
    "\n",
    "\n",
    "modelo_stepwise(x_train, x_test, y_train, y_test, \"backward\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "optb = OptimalBinning(name=\"Edad\", dtype=\"numerical\", max_n_bins=10) \n",
    "optb.fit(df_train_total[\"Edad\"], df_train_total[\"Default\"])\n",
    "tabla = optb.binning_table.build()\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivs = []\n",
    "\n",
    "for col in variables_numericas_total:\n",
    "    optb = OptimalBinning(name=col, dtype=\"numerical\", max_n_bins=10)\n",
    "    optb.fit(df_train_total[col], df_train_total[\"Default\"])\n",
    "    \n",
    "    tabla = optb.binning_table.build()\n",
    "    # Opción 1: sumar IV de todos los bins\n",
    "    iv = tabla[\"IV\"].sum()\n",
    "    # Opción 2 (más prolija): tomar la fila Totals si aparece\n",
    "    # iv = tabla.loc[\"Totals\", \"Information value\"]\n",
    "    \n",
    "    ivs.append((col, iv))\n",
    "\n",
    "ranking_iv = sorted(ivs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "for col, iv in ranking_iv:\n",
    "    print(f\"{col} -> IV={iv:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d5624",
   "metadata": {},
   "source": [
    "### Modelo final 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a75abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_woe = pd.DataFrame()\n",
    "\n",
    "def crear_df_woe(lista_variables):\n",
    "\n",
    "\n",
    "    for col in lista_variables:\n",
    "\n",
    "        if col != \"Nivel_Educacional\":\n",
    "\n",
    "            optb = OptimalBinning(\n",
    "                name=col,\n",
    "                dtype=\"numerical\",\n",
    "                max_n_bins=20\n",
    "            )\n",
    "            \n",
    "            optb.fit(\n",
    "                df_train_total[col].values,\n",
    "                df_train_total[\"Default\"].values,\n",
    "                sample_weight=peso_train2\n",
    "            )\n",
    "            \n",
    "            woe_values = optb.transform(df_train_total[col].values, metric=\"woe\")\n",
    "            \n",
    "            df_woe[col] = woe_values\n",
    "        \n",
    "        else:\n",
    "    \n",
    "            col_cat = col\n",
    "\n",
    "            optb_cat = OptimalBinning(\n",
    "                name=col_cat,\n",
    "                dtype=\"categorical\"     # <-- IMPORTANTE\n",
    "            )\n",
    "\n",
    "            optb_cat.fit(\n",
    "                df_train_total[col_cat].astype(str).values,     # por seguridad => str\n",
    "                df_train_total[\"Default\"].values,\n",
    "                sample_weight=peso_train2\n",
    "            )\n",
    "\n",
    "            woe_cat = optb_cat.transform(\n",
    "            df_train_total[col_cat].astype(str).values,\n",
    "            metric=\"woe\"\n",
    "            )\n",
    "\n",
    "            df_woe[col_cat] = woe_cat   # agregar al dataframe final\n",
    "\n",
    "\n",
    "    df_woe[\"Default\"] = df_train_total[\"Default\"].values\n",
    "\n",
    "    return df_woe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981940e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_pivote = df_train.columns[:-1]\n",
    "df_woe = crear_df_woe(variables_pivote)\n",
    "\n",
    "df_woe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21781db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_total[\"Ratio_Años_Trabajando_Deuda_Total\"] = df_train_total[\"Ratio_Años_Trabajando_Deuda_Comercial\"] + df_train_total[\"Ratio_Años_Trabajando_Deuda_Credito\"] + df_train_total[\"Ratio_Años_Trabajando_Otras_Deudas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd59493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables Jonathan\n",
    "\n",
    "variables_pivote = [\"Edad\", \"Deuda_Comercial\", \"Deuda_Credito\", \"Años_Trabajando\", \"Ratio_Años_Trabajando_Deuda_Total\"]\n",
    "df_woe = crear_df_woe(variables_pivote)\n",
    "\n",
    "x_train = df_woe[variables_pivote]\n",
    "y_train = df_woe[\"Default\"]\n",
    "\n",
    "x_test = df_woe[variables_pivote]\n",
    "y_test = df_woe[\"Default\"]\n",
    "\n",
    "\n",
    "def modelo_stepwise(x_train, x_test, y_train, y_test, direccion):\n",
    "    modelo = LogisticRegression()\n",
    "    stepwise = SequentialFeatureSelector(\n",
    "        modelo,\n",
    "        direction = direccion,\n",
    "        scoring = \"accuracy\",\n",
    "        cv=5,\n",
    "\n",
    "    )\n",
    "\n",
    "    stepwise.fit(x_train, y_train)\n",
    "    filtro = stepwise.get_support()\n",
    "    col_stepwise = x_train.columns[filtro]\n",
    "    modelo.fit(x_train[col_stepwise], y_train, sample_weight= peso_train2)\n",
    "    y_pred = modelo.predict(x_test[col_stepwise])\n",
    "    y_scores = modelo.predict_proba(x_test[col_stepwise])[:, 1]\n",
    "\n",
    "    threshold_manual = 0.36\n",
    "\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "    print(f\"Valor de Accuracy: {accuracy}\")\n",
    "    print(\"Variables seleccionadas con RFE:\")\n",
    "    print(col_stepwise)\n",
    "    \n",
    "    coef = modelo.coef_[0]\n",
    "    intercepto = modelo.intercept_[0]\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "    \"Variable\": col_stepwise,\n",
    "    \"Coeficiente\": coef})\n",
    "\n",
    "    print(\"Intercepto:\", intercepto)\n",
    "    print(df_coef)\n",
    "\n",
    "    thresholds = np.arange(0, 1.01, 0.01)   # thresholds de 0.00 a 1.00\n",
    "    accuracies = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred_t = (y_scores >= t).astype(int)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred_t))\n",
    "\n",
    "        best_t = thresholds[np.argmax(accuracies)]\n",
    "        best_acc = max(accuracies)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Con el corte optimo {best_t} el Accuracy es: {best_acc}\")\n",
    "\n",
    "    return y_scores, y_pred\n",
    "\n",
    "y_scores, y_pred = modelo_stepwise(x_train, x_test, y_train, y_test, \"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_woe = pd.DataFrame()\n",
    "\n",
    "def crear_df_woe(lista_variables):\n",
    "\n",
    "\n",
    "    for col in lista_variables:\n",
    "\n",
    "        if col != \"Nivel_Educacional\":\n",
    "\n",
    "            optb = OptimalBinning(\n",
    "                name=col,\n",
    "                dtype=\"numerical\",\n",
    "                max_n_bins=10\n",
    "            )\n",
    "            \n",
    "            optb.fit(\n",
    "                df_train_total[col].values,\n",
    "                df_train_total[\"Default\"].values,\n",
    "                sample_weight=peso_train2\n",
    "            )\n",
    "            \n",
    "            woe_values = optb.transform(df_train_total[col].values, metric=\"woe\")\n",
    "            \n",
    "            df_woe[col] = woe_values\n",
    "        \n",
    "        else:\n",
    "    \n",
    "            col_cat = col\n",
    "\n",
    "            optb_cat = OptimalBinning(\n",
    "                name=col_cat,\n",
    "                dtype=\"categorical\"     # <-- IMPORTANTE\n",
    "            )\n",
    "\n",
    "            optb_cat.fit(\n",
    "                df_train_total[col_cat].astype(str).values,     # por seguridad => str\n",
    "                df_train_total[\"Default\"].values,\n",
    "                sample_weight=peso_train2\n",
    "            )\n",
    "\n",
    "            woe_cat = optb_cat.transform(\n",
    "            df_train_total[col_cat].astype(str).values,\n",
    "            metric=\"woe\"\n",
    "            )\n",
    "\n",
    "            df_woe[col_cat] = woe_cat   # agregar al dataframe final\n",
    "\n",
    "\n",
    "    df_woe[\"Default\"] = df_train_total[\"Default\"].values\n",
    "\n",
    "    return df_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_total[\"Ratio_Años_Trabajando_Deuda_Total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65468763",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_pivote = [\n",
    "    \"Ratio_Años_Trabajando_Deuda_Total\",\n",
    "    \"Ratio_Edad_Deuda_Comercial\",\n",
    "    \"Ratio_Edad_Ratio_Ingresos_Deudas\",\n",
    "    \"Ratio_Años_Trabajando_Ratio_Ingresos_Deudas\",\n",
    "    \"Prod_Deuda_Comercial_Ratio_Ingresos_Deudas\",\n",
    "    \"Ratio_Ingresos_Deudas\",\n",
    "    \"Ratio_Ingresos_Deudas_cuadrado\",\n",
    "    \"Prod_Deuda_Credito_Ratio_Ingresos_Deudas\",\n",
    "    \"Ratio_Ingresos_Deuda_Credito\",\n",
    "    \"Ratio_Ingresos_Deuda_Comercial\",\n",
    "    \"Deuda_Comercial\",\n",
    "    \"Deuda_Comercial_cuadrado\",\n",
    "    \"Prod_Edad_Años_Trabajando\",\n",
    "    \"Ratio_Años_Trabajando_Ingresos\",\n",
    "    \"Ratio_Ingresos_Ratio_Ingresos_Deudas\",\n",
    "    \"Años_Trabajando\",\n",
    "    \"Años_Trabajando_cuadrado\",\n",
    "    \"Ratio_Edad_Años_Trabajando\",\n",
    "    \"Ratio_Edad_Deuda_Credito\",\n",
    "    \"Edad\",\n",
    "    \"Edad_cuadrado\",\n",
    "    \"Prod_Años_Trabajando_Ingresos\",\n",
    "    \"Prod_Deuda_Comercial_Deuda_Credito\",\n",
    "    \"Prod_Ingresos_Ratio_Ingresos_Deudas\",\n",
    "    \"Prod_Otras_Deudas_Ratio_Ingresos_Deudas\",\n",
    "    \"Ratio_Ingresos_Otras_Deudas\",\n",
    "    \"Prod_Edad_Ratio_Ingresos_Deudas\",\n",
    "    \"Ratio_Edad_Otras_Deudas\"\n",
    "]\n",
    "\n",
    "#variables_pivote = [\"Ratio_Años_Trabajando_Deuda_Total\", \"Ingresos\", \"Nivel_Educacional\", \"Deuda_Comercial\" ]#, \"Prod_Deuda\"]# \"Edad\", \"Deuda_Credito\", \"Años_Trabajando\", \"Nivel_Educacional\"]\n",
    "df_woe = crear_df_woe(variables_pivote)\n",
    "x_train = df_woe[variables_pivote]\n",
    "y_train = df_woe[\"Default\"]\n",
    "\n",
    "x_test = df_woe[variables_pivote]\n",
    "y_test = df_woe[\"Default\"]\n",
    "\n",
    "RL = LogisticRegression(penalty= \"l2\")\n",
    "\n",
    "RL.fit(x_train, y_train, sample_weight=peso_train2)\n",
    "y_predic = RL.predict(x_test)\n",
    "y_scores = RL.predict_proba(x_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_predic)\n",
    "print(\"El accuracy del modelo es\")\n",
    "print(f\"El Accuracy del modelo es {acc:.3f}\")\n",
    "\n",
    "\n",
    "coef = RL.coef_[0]\n",
    "intercepto = RL.intercept_[0]\n",
    "\n",
    "df_coef = pd.DataFrame({\n",
    "    \"Variable\": variables_pivote,\n",
    "    \"Coeficiente\": coef\n",
    "})\n",
    "\n",
    "print(\"Intercepto:\", intercepto)\n",
    "print(df_coef)\n",
    "\n",
    "thresholds = np.arange(0, 1.01, 0.01)   # thresholds de 0.00 a 1.00\n",
    "accuracies = []\n",
    "\n",
    "for t in thresholds:\n",
    "        y_pred_t = (y_scores >= t).astype(int)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred_t))\n",
    "\n",
    "        best_t = thresholds[np.argmax(accuracies)]\n",
    "        best_acc = max(accuracies)\n",
    "\n",
    "\n",
    "print(f\"Con el corte optimo {best_t} el Accuracy es: {best_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_pivote = [\"Edad\", \"Deuda_Credito\", \"Años_Trabajando\", \"Nivel_Educacional\", \"Ratio_Años_Trabajando_Deuda_Total\"]\n",
    "df_woe = crear_df_woe(variables_pivote)\n",
    "df_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_pivote = [\"Edad\", \"Deuda_Credito\", \"Años_Trabajando\", \"Nivel_Educacional\", \"Ratio_Años_Trabajando_Deuda_Total\"]\n",
    "df_woe = crear_df_woe(variables_pivote)\n",
    "\n",
    "x_train = df_woe[variables_pivote]\n",
    "y_train = df_woe[\"Default\"]\n",
    "\n",
    "x_test = df_woe[variables_pivote]\n",
    "y_test = df_woe[\"Default\"]\n",
    "\n",
    "RL = LogisticRegression()\n",
    "\n",
    "RL.fit(x_train, y_train, sample_weight=peso_train2)\n",
    "y_predic = RL.predict(x_test)\n",
    "y_scores = RL.predict_proba(x_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_predic)\n",
    "print(\"El accuracy del modelo es\")\n",
    "print(f\"El Accuracy del modelo es {acc:.3f}\")\n",
    "\n",
    "\n",
    "coef = RL.coef_[0]\n",
    "intercepto = RL.intercept_[0]\n",
    "\n",
    "df_coef = pd.DataFrame({\n",
    "    \"Variable\": variables_pivote,\n",
    "    \"Coeficiente\": coef\n",
    "})\n",
    "\n",
    "print(\"Intercepto:\", intercepto)\n",
    "print(df_coef)\n",
    "\n",
    "thresholds = np.arange(0, 1.01, 0.01)   # thresholds de 0.00 a 1.00\n",
    "accuracies = []\n",
    "\n",
    "for t in thresholds:\n",
    "        y_pred_t = (y_scores >= t).astype(int)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred_t))\n",
    "\n",
    "        best_t = thresholds[np.argmax(accuracies)]\n",
    "        best_acc = max(accuracies)\n",
    "\n",
    "\n",
    "print(f\"Con el corte optimo {best_t} el Accuracy es: {best_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39309fd7",
   "metadata": {},
   "source": [
    "# Modelo Final Grupo (INFORME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba43be1",
   "metadata": {},
   "source": [
    "En el siguiente codigo se utilizaron todas las herramientas que como grupo pudimos interiorizar anteriormente, las cuales consideran, creacion de variables, transformacion a WOE, seleccion de variables, optimizacion de threshold todo con el fin de entregar el mejor modelo y con el accuracy que como grupo logramos desarrollar  \n",
    "\n",
    "Es por lo anterior que se vuelve a realizar un modelo y se vuelve a cargar la data desde 0, esto porque generamos un codigo final más limpio y sencillo de leer para que fuera evaluado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crear_woes(x_train, target):\n",
    "    \"\"\"Calcula WOE sobre una serie y devuelve el mapa y los bins usados\"\"\"\n",
    "    _, bins = pd.qcut(x_train, q=10, duplicates='drop', retbins=True)\n",
    "    bins[0] = -np.inf\n",
    "    bins[-1] = np.inf\n",
    "\n",
    "    # Aplicar bins para calcular WOE\n",
    "    bineado = pd.cut(x_train, bins=bins).astype(str)\n",
    "\n",
    "    # Tabla WOE\n",
    "    df_temp = pd.DataFrame({'bin': bineado, 'target': target})\n",
    "    grouped = df_temp.groupby('bin')['target'].agg(['count', 'sum'])\n",
    "    grouped['malos_pagadores'] = grouped['sum'].replace(0, 0.5)\n",
    "    grouped['buenos_pagadores'] = (grouped['count'] - grouped['sum']).replace(0, 0.5)\n",
    "    \n",
    "    total_malos = target.sum()\n",
    "    total_buenos = target.count() - total_malos\n",
    "    \n",
    "    grouped['WOE'] = np.log((grouped['buenos_pagadores'] / total_buenos) / (grouped['malos_pagadores'] / total_malos))\n",
    "    grouped['IV'] = (grouped['buenos_pagadores']/total_buenos - grouped['malos_pagadores']/total_malos) * grouped['WOE']\n",
    "    \n",
    "    return grouped['WOE'].to_dict(), grouped['IV'].sum(), bins\n",
    "\n",
    "def transformar_a_woe(x_train, tabla_woes, bins):\n",
    "    \"\"\"Aplica el WOE a nuevos datos\"\"\"\n",
    "    bineado = pd.cut(x_train, bins=bins).astype(str)\n",
    "    return bineado.map(tabla_woes).fillna(0) # Si hay nulos (categoría nueva), WOE 0\n",
    "\n",
    "\n",
    "def regresion_y_metricas(df, nombre_de_la_prueba):\n",
    "    print(f' ============ Estrategia {nombre_de_la_prueba} ============')\n",
    "    \n",
    "    if \"Id_Cliente\" in df.columns: df.drop([\"Id_Cliente\"], axis=1, inplace=True)\n",
    "    if \"Ratio_Ingresos_Deudas\" in df.columns: df.drop([\"Ratio_Ingresos_Deudas\"], axis=1, inplace=True)\n",
    "\n",
    "    target = 'Default'\n",
    "\n",
    "    # Aplicacion del split en crudo por ahroa, ya que posteriormente se pasaran a woe\n",
    "    x_en_bruto = df.drop(columns=[target])\n",
    "    y_en_bruto = df[target]\n",
    "    x_train_bruto, X_test_bruto, y_train, y_test = train_test_split(x_en_bruto, y_en_bruto, test_size=0.3, random_state=42, stratify=y_en_bruto)\n",
    "\n",
    "    # Aplicacion del woe e IV\n",
    "    print(\"Calculando WOE\")\n",
    "\n",
    "    X_train_woe = pd.DataFrame(index=x_train_bruto.index)\n",
    "    X_test_woe = pd.DataFrame(index=X_test_bruto.index)\n",
    "    iv_resumen = {}\n",
    "\n",
    "    for col in x_train_bruto.columns:\n",
    "        # acalculo del woe solo al train, evitamos fuga de informacion\n",
    "        tabla_woes, iv, bins = crear_woes(x_train_bruto[col], y_train)\n",
    "        iv_resumen[col] = iv\n",
    "            \n",
    "        # luego del calculo sobre test se aplica el woe\n",
    "        X_train_woe[col] = transformar_a_woe(x_train_bruto[col], tabla_woes, bins)\n",
    "        X_test_woe[col] = transformar_a_woe(X_test_bruto[col], tabla_woes, bins)\n",
    "\n",
    "    print(f\"Variables transformadas: {X_train_woe.shape[1]}\")\n",
    "\n",
    "    #ocupamos el IV para descartar variables con IV bajo\n",
    "    iv_umbral = 0.01\n",
    "    variables_fuertes_iv = [k for k, v in iv_resumen.items() if v >= iv_umbral]\n",
    "    print(f\"Variables con IV > {iv_umbral}: {len(variables_fuertes_iv)}\")\n",
    "\n",
    "    X_train_woe = X_train_woe[variables_fuertes_iv]\n",
    "    X_test_woe = X_test_woe[variables_fuertes_iv]\n",
    "\n",
    "    print(\"\\nResumen del IV:\")\n",
    "    iv_df = pd.DataFrame(list(iv_resumen.items()), columns=['Variable', 'IV']).sort_values('IV', ascending=False)\n",
    "    print(iv_df)\n",
    "    \n",
    "    RL = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    RL.fit(X_train_woe, y_train)\n",
    "    y_predic = RL.predict(X_test_woe)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_predic)\n",
    "    print(f\"\\n{nombre_de_la_prueba} El Accuracy del modelo es {acc:.3f}\")\n",
    "\n",
    "    coef = RL.coef_[0]\n",
    "    intercepto = RL.intercept_[0]\n",
    "\n",
    "    df_coef = pd.DataFrame({\n",
    "        \"Variable\": list(X_train_woe.columns),\n",
    "        \"Coeficiente\": coef\n",
    "    })\n",
    "\n",
    "    print(\"Intercepto:\", intercepto)\n",
    "    print(df_coef)\n",
    "\n",
    "    y_prob = RL.predict_proba(X_test_woe)[:, 1]\n",
    "    thresholds = np.arange(0, 1.01, 0.01) \n",
    "    accuracies = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred_t = (y_prob >= t).astype(int)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred_t)) \n",
    "\n",
    "    mejor_t = thresholds[np.argmax(accuracies)]\n",
    "    mejor_accuracy = max(accuracies)\n",
    "\n",
    "    print(f\"\\nMejor threshold = {mejor_t:.2f}\")\n",
    "    print(f\"Accuracy máximo = {mejor_accuracy:.4f}\")\n",
    "\n",
    "    #Probar validacion cruzada para validar el accuracy\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    cv_prob = cross_val_score(RL, X_train_woe, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "    print(\"\\n----- Resultados de Validación Cruzada (10 Folds) ----\")\n",
    "    print(f\"Accuracy de cada intento: {cv_prob}\")\n",
    "    print(f\"\\nAccuracy Promedio Real: {cv_prob.mean():.4f}\")\n",
    "    print(f\"Desviación Estándar: {cv_prob.std():.4f}\")\n",
    "\n",
    "    print(\"\\ninterpretación:\")\n",
    "    print(f\"modelo oscila entre {(cv_prob.mean() - cv_prob.std()):.4f} y {(cv_prob.mean() + cv_prob.std()):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2e53a",
   "metadata": {},
   "source": [
    "## Creacion de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686174",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10\n",
    "\n",
    "df[\"Total_Deudas\"] = df[\"Deuda_Comercial\"] + df[\"Deuda_Credito\"] + df[\"Otras_Deudas\"]\n",
    "df['Ratio_Total_Deudas_Edad'] = df['Total_Deudas'] / (df['Edad'] + eps)\n",
    "df['Ratio_Años_Trabajando_Deuda_Credito'] = df['Años_Trabajando'] / (df['Deuda_Credito'] + eps)\n",
    "df['Prod_Edad_Ingresos'] = df['Edad'] * df['Ingresos']\n",
    "df['Ratio_Edad_Deuda_Credito'] = df['Edad'] / (df['Deuda_Credito'] + eps)\n",
    "df['Prod_Años_Trabajando_Deuda_Comercial'] = df['Años_Trabajando'] * df['Deuda_Comercial']\n",
    "df['Prod_Ingresos_Deuda_Comercial'] = df['Ingresos'] * df['Deuda_Comercial']\n",
    "df['Ratio_Ingresos_Edad'] = df['Ingresos'] / (df['Edad'] + eps)\n",
    "df['Prod_Otras_Deudas_Total_Deudas'] = df['Otras_Deudas'] * df['Total_Deudas']\n",
    "df['Ratio_Edad_Total_Deudas'] = df['Edad'] / (df['Total_Deudas'] + eps)\n",
    "df['Ratio_Total_Deudas_Deuda_Credito'] = df['Total_Deudas'] / (df['Deuda_Credito'] + eps)\n",
    "df['Ratio_Otras_Deudas_Total_Deudas'] = df['Otras_Deudas'] / (df['Total_Deudas'] + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_elegidas = ['Ratio_Total_Deudas_Edad',\n",
    " 'Ratio_Años_Trabajando_Deuda_Credito',\n",
    " 'Prod_Edad_Ingresos',\n",
    " 'Ratio_Edad_Deuda_Credito',\n",
    " 'Prod_Años_Trabajando_Deuda_Comercial',\n",
    " 'Prod_Ingresos_Deuda_Comercial',\n",
    " 'Ratio_Ingresos_Edad',\n",
    " 'Prod_Otras_Deudas_Total_Deudas',\n",
    " 'Ratio_Edad_Total_Deudas',\n",
    " 'Ratio_Total_Deudas_Deuda_Credito',\n",
    " 'Ratio_Otras_Deudas_Total_Deudas',\n",
    " 'Default']\n",
    "\n",
    "df_features = df[features_elegidas]\n",
    " \n",
    "regresion_y_metricas(df_features, 'Mejores features: WOE + Stepwise(Variables elegidas con backward)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b810126",
   "metadata": {},
   "source": [
    "## Pasos para llegar a las features seleccionadas\n",
    "\n",
    "1. Se hace una combinacion de variables para crear nuevas features\n",
    "2. Se calcula el WOE con train y luego se transforma\n",
    "3. Se evalua el IV para descartar variables\n",
    "4. Se aplica regresion logistica sobre el modelo con woes\n",
    "    \n",
    "    4.1. Se evaluan las features con forward\n",
    "\n",
    "    4.2. Se evaluan las features con backward\n",
    "\n",
    "    4.3. Se evaluan las features bidireccionalmente con forward y backward\n",
    "    \n",
    "    4.4. Se evaluan las features con Ridge(L2) y Lasso(L1)\n",
    "    \n",
    "    4.5. Se comparan los accuracy y se elige la mejor estrategia\n",
    "\n",
    "    4.6. De la mejor estrategia se optimiza el threshold\n",
    "\n",
    "5. Se muestra la mejor estrategia, las mejores features y el accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"Tabla Trabajo Grupal N°2.xlsx\", sheet_name= \"Desarrollo\")\n",
    "\n",
    "df.drop([\"Id_Cliente\"], axis=1, inplace=True)\n",
    "df.drop([\"Ratio_Ingresos_Deudas\"], axis=1, inplace=True)\n",
    "df.drop([\"Nivel_Educacional\"], axis=1, inplace=True)\n",
    "\n",
    "# Dropeamos el ratio ingersos deduas, ya que se volverán a generar al momento de generar ratios\n",
    "df[\"Total_Deudas\"] = df[\"Deuda_Comercial\"] + df[\"Deuda_Credito\"] + df[\"Otras_Deudas\"]\n",
    "target = 'Default'\n",
    "\n",
    "# Generar Ratios\n",
    "col_continuas = [col for col in df.columns if df[col].dtype in [\"int64\", \"float64\"] and col != target]\n",
    "\n",
    "def combinacion_de_variables(df, variables):\n",
    "    df_2 = df.copy()\n",
    "    eps = 1e-6 \n",
    "    for x, y in combinations(variables, 2):\n",
    "        df_2[f\"Prod_{x}_{y}\"] = df[x] * df[y]\n",
    "        df_2[f\"Ratio_{x}_{y}\"] = df[x] / (df[y] + eps)\n",
    "        df_2[f\"Ratio_{y}_{x}\"] = df[y] / (df[x] + eps)\n",
    "    return df_2\n",
    "\n",
    "df = combinacion_de_variables(df, col_continuas)\n",
    "\n",
    "# Aplicacion del split en crudo por ahroa, ya que posteriormente se pasaran a woe\n",
    "x_en_bruto = df.drop(columns=[target])\n",
    "y_en_bruto = df[target]\n",
    "x_train_bruto, X_test_bruto, y_train, y_test = train_test_split(x_en_bruto, y_en_bruto, test_size=0.3, random_state=42, stratify=y_en_bruto)\n",
    "\n",
    "# Aplicacion del woe e IV\n",
    "def crear_woes(x_train, target):\n",
    "    \"\"\"Calcula WOE sobre una serie y devuelve el mapa y los bins usados\"\"\"\n",
    "    _, bins = pd.qcut(x_train, q=10, duplicates='drop', retbins=True)\n",
    "    bins[0] = -np.inf\n",
    "    bins[-1] = np.inf\n",
    "\n",
    "    # Aplicar bins para calcular WOE\n",
    "    bineado = pd.cut(x_train, bins=bins).astype(str)\n",
    "\n",
    "    # Tabla WOE\n",
    "    df_temp = pd.DataFrame({'bin': bineado, 'target': target})\n",
    "    grouped = df_temp.groupby('bin')['target'].agg(['count', 'sum'])\n",
    "    grouped['malos_pagadores'] = grouped['sum'].replace(0, 0.5)\n",
    "    grouped['buenos_pagadores'] = (grouped['count'] - grouped['sum']).replace(0, 0.5)\n",
    "    \n",
    "    total_malos = target.sum()\n",
    "    total_buenos = target.count() - total_malos\n",
    "    \n",
    "    grouped['WOE'] = np.log((grouped['buenos_pagadores'] / total_buenos) / (grouped['malos_pagadores'] / total_malos))\n",
    "    grouped['IV'] = (grouped['buenos_pagadores']/total_buenos - grouped['malos_pagadores']/total_malos) * grouped['WOE']\n",
    "    \n",
    "    return grouped['WOE'].to_dict(), grouped['IV'].sum(), bins\n",
    "\n",
    "def transformar_a_woe(x_train, tabla_woes, bins):\n",
    "    \"\"\"Aplica el WOE a nuevos datos\"\"\"\n",
    "    bineado = pd.cut(x_train, bins=bins).astype(str)\n",
    "    return bineado.map(tabla_woes).fillna(0) # Si hay nulos (categoría nueva), WOE 0\n",
    "\n",
    "# --- Aplicar transformación\n",
    "print(\"Calculando WOE\")\n",
    "\n",
    "X_train_woe = pd.DataFrame(index=x_train_bruto.index)\n",
    "X_test_woe = pd.DataFrame(index=X_test_bruto.index)\n",
    "iv_resumen = {}\n",
    "\n",
    "for col in x_train_bruto.columns:\n",
    "    # acalculo woe solo al train, evitamos fuga de informacion\n",
    "    tabla_woes, iv, bins = crear_woes(x_train_bruto[col], y_train)\n",
    "    iv_resumen[col] = iv\n",
    "        \n",
    "    # luego del calculo sobre test se aplica el woe\n",
    "    X_train_woe[col] = transformar_a_woe(x_train_bruto[col], tabla_woes, bins)\n",
    "    X_test_woe[col] = transformar_a_woe(X_test_bruto[col], tabla_woes, bins)\n",
    "\n",
    "print(f\"Variables transformadas: {X_train_woe.shape[1]}\")\n",
    "\n",
    "#ocupamos el IV para descartar variables con IV bajo\n",
    "iv_umbral = 0.01\n",
    "variables_fuertes_iv = [k for k, v in iv_resumen.items() if v >= iv_umbral]\n",
    "print(f\"Variables con IV > {iv_umbral}: {len(variables_fuertes_iv)}\")\n",
    "\n",
    "X_train_woe = X_train_woe[variables_fuertes_iv]\n",
    "X_test_woe = X_test_woe[variables_fuertes_iv]\n",
    "\n",
    "\n",
    "#APLICACION DE ESTRATEGIAS\n",
    "\n",
    "# --- A) FORWARD\n",
    "def aplicar_forward(X_tr, y_tr, X_te, y_te):\n",
    "    inicial, restante = [], list(X_tr.columns)\n",
    "    mejor_accuracy = 0\n",
    "    print(\"\\n----- Forward -----\")\n",
    "    while restante:\n",
    "        cv_prob = []\n",
    "        for col in restante:\n",
    "            m = LogisticRegression(solver='liblinear', random_state=42)\n",
    "            m.fit(X_tr[inicial + [col]], y_tr)\n",
    "            cv_prob.append((accuracy_score(y_te, m.predict(X_te[inicial + [col]])), col))\n",
    "        cv_prob.sort(reverse=True)\n",
    "        if cv_prob[0][0] > mejor_accuracy:\n",
    "            mejor_accuracy, mejor_col = cv_prob[0]\n",
    "            inicial.append(mejor_col)\n",
    "            restante.remove(mejor_col)\n",
    "            print(f\"Agregada: {cv_prob[0][1]} (Acc: {mejor_accuracy:.4f})\")\n",
    "        else: break\n",
    "    return inicial, mejor_accuracy\n",
    "\n",
    "# --- B) BACKWARD CON TOLERANCIA\n",
    "def aplicar_backward(X_tr, y_tr, X_te, y_te, tolerancia=0.001):\n",
    "    \"\"\"\n",
    "    Elimina variables si el accuracy no baja más que la torelancia.\n",
    "    tolerancia=0.001 significa que aceptamos perder un 0.01% de accuracy a cambio de quitar una variable.\n",
    "    \"\"\"\n",
    "    cols = list(X_tr.columns)\n",
    "    m = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    m.fit(X_tr[cols], y_tr)\n",
    "    mejor_accuracy = accuracy_score(y_te, m.predict(X_te[cols]))\n",
    "    \n",
    "    print(f\"\\n--- Backward (Tolerancia: {tolerancia}) ---\")\n",
    "    print(f\"Inicio: {len(cols)} vars | Acc: {mejor_accuracy:.4f}\")\n",
    "    \n",
    "    loop = True\n",
    "    while loop and len(cols) > 1:\n",
    "        cv_prob = []\n",
    "        # Evaluar qué pasa si quitamos cada variable\n",
    "        for col in cols:\n",
    "            temporal_col = [f for f in cols if f != col]\n",
    "            m.fit(X_tr[temporal_col], y_tr)\n",
    "            acc = accuracy_score(y_te, m.predict(X_te[temporal_col]))\n",
    "            cv_prob.append((acc, col))\n",
    "        \n",
    "        cv_prob.sort(reverse=True)\n",
    "        accuracy_sin_var, variables_quitadas = cv_prob[0]\n",
    "        \n",
    "        # Si el accuracy al quitar la variable es MAYOR o IGUAL a (Accuracy Actual - Tolerancia)\n",
    "        # Significa que la pérdida es aceptable, así que la quitamos\n",
    "        if accuracy_sin_var >= (mejor_accuracy - tolerancia):\n",
    "            cols.remove(variables_quitadas)\n",
    "            # Solo actualizamos el mejor_accuracy si realmente mejoró\n",
    "            # pero aceptamos el nuevo estado más simple.\n",
    "            if accuracy_sin_var > mejor_accuracy:\n",
    "                mejor_accuracy = accuracy_sin_var\n",
    "            print(f\"Eliminada: {variables_quitadas} (Acc: {accuracy_sin_var:.4f})\")\n",
    "        else:\n",
    "            print(\"No se puede eliminar más sin sacrificar accuracy\")\n",
    "            loop = False\n",
    "            \n",
    "    return cols, mejor_accuracy\n",
    "\n",
    "# --- C) STEPWISE BIDIRECCIONAL---\n",
    "def stepwise_bidireccional(X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"Agrega la mejor, luego intenta eliminar la peor\"\"\"\n",
    "    col_actual = []\n",
    "    restante = list(X_tr.columns)\n",
    "    mejor_accuracy = 0\n",
    "    \n",
    "    print(\"\\n----- Stepwise bidireccional-----\")\n",
    "    while restante:\n",
    "        # 1. Paso Forward\n",
    "        acc_agregados = []\n",
    "        for col in restante:\n",
    "            m = LogisticRegression(solver='liblinear', random_state=42)\n",
    "            m.fit(X_tr[col_actual + [col]], y_tr)\n",
    "            acc = accuracy_score(y_te, m.predict(X_te[col_actual + [col]]))\n",
    "            acc_agregados.append((acc, col))\n",
    "        acc_agregados.sort(reverse=True)\n",
    "        \n",
    "        if acc_agregados[0][0] > mejor_accuracy:\n",
    "            mejor_accuracy, agregado = acc_agregados[0]\n",
    "            col_actual.append(agregado)\n",
    "            restante.remove(agregado)\n",
    "            print(f\"Agregada: {agregado} (Acc: {mejor_accuracy:.4f})\")\n",
    "            \n",
    "            # 2. Paso Backward (quita variables)\n",
    "            # Revisamos si alguna de las QUE YA TENEMOS sobra\n",
    "            if len(col_actual) > 2:\n",
    "                acc_quitados = []\n",
    "                for col in col_actual:\n",
    "                    temp = [f for f in col_actual if f != col]\n",
    "                    m.fit(X_tr[temp], y_tr)\n",
    "                    acc = accuracy_score(y_te, m.predict(X_te[temp]))\n",
    "                    acc_quitados.append((acc, col))\n",
    "                acc_quitados.sort(reverse=True)\n",
    "                \n",
    "                # Si al quitar una mejoramos el mejor_accuracy actual, la sacamos\n",
    "                if acc_quitados[0][0] > mejor_accuracy:\n",
    "                    mejor_accuracy, quitados = acc_quitados[0]\n",
    "                    col_actual.remove(quitados)\n",
    "                    restante.append(quitados)\n",
    "                    print(f\"Eliminada por redundancia: {quitados} (Acc subió a: {mejor_accuracy:.4f})\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return col_actual, mejor_accuracy\n",
    "\n",
    "\n",
    "mejor_fwd, acc_fwd = aplicar_forward(X_train_woe, y_train, X_test_woe, y_test)\n",
    "\n",
    "mejor_bwd, acc_bwd = aplicar_backward(X_train_woe, y_train, X_test_woe, y_test, tolerancia=0.0005)\n",
    "\n",
    "mejor_bidir, acc_bidir = stepwise_bidireccional(X_train_woe, y_train, X_test_woe, y_test)\n",
    "\n",
    "# --- RESULTADOS\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" COMPARATIVA FINAL\")\n",
    "print(\"=\"*30)\n",
    "print(f\"1. Forward:        {acc_fwd:.4f} ({len(mejor_fwd)} variables)\")\n",
    "print(f\"2. Backward:       {acc_bwd:.4f} ({len(mejor_bwd)} variables)\")\n",
    "print(f\"3. Bidireccional:  {acc_bidir:.4f} ({len(mejor_bidir)} va   riables)\")\n",
    "\n",
    "# guarda y luego muestra resultados de los intentos y los ordenamos de mayor a menor\n",
    "resultados = [\n",
    "    ('Forward', acc_fwd, len(mejor_fwd), mejor_fwd),\n",
    "    ('Backward Smart', acc_bwd, len(mejor_bwd), mejor_bwd),\n",
    "    ('Bidireccional', acc_bidir, len(mejor_bidir), mejor_bidir)\n",
    "]\n",
    "\n",
    "# Ordenamos por Accuracy descendente\n",
    "resultados.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "nombre_ganador, acc_ganador, ganadores_var_len, variables_ganadores = resultados[0]\n",
    "print(f\"\\n>>> Estrategia ganadora: {nombre_ganador}\")\n",
    "\n",
    "# ENTRENAMIENTO FINAL\n",
    "modelo_final = LogisticRegression(solver='liblinear', random_state=42)\n",
    "modelo_final.fit(X_train_woe[variables_ganadores], y_train)\n",
    "\n",
    "# Optimización Threshold\n",
    "probs = modelo_final.predict_proba(X_test_woe[variables_ganadores])[:, 1]\n",
    "mejor_t = 0.5; max_acc = 0\n",
    "for t in np.arange(0.1, 0.9, 0.01):\n",
    "    acc = accuracy_score(y_test, (probs >= t).astype(int))\n",
    "    if acc > max_acc: max_acc = acc; mejor_t = t\n",
    "\n",
    "print(f\"Accuracy Optimizado: {max_acc:.4f} (Threshold: {mejor_t:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d5098",
   "metadata": {},
   "source": [
    "# Modelo Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2be302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
